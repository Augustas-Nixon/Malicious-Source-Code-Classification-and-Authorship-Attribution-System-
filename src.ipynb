from datasets import load_dataset

# Login using e.g. `huggingface-cli login` to access this dataset
ds = load_dataset("basakdemirok/AIGCodeSet")

print(ds)

# Count distribution of labels in train split
label_counts = ds["train"].to_pandas()["label"].value_counts()
print(label_counts)

print(ds)

# Count distribution of labels in train split
label_counts = ds["test"].to_pandas()["label"].value_counts()
print(label_counts)

import pandas as pd
import ast
import numpy as np
from huggingface_hub import hf_hub_download
import pandas as pd

!wget -O datasetllm.csv "https://huggingface.co/datasets/basakdemirok/AIGCodeSet/resolve/main/data/created_dataset_with_llms.csv"

llm = pd.read_csv("datasetllm.csv")

llm

!wget -O datasethuman.csv "https://huggingface.co/datasets/basakdemirok/AIGCodeSet/resolve/main/data/human_selected_dataset.csv"

human = pd.read_csv("datasethuman.csv")

human

from openai import OpenAI
from google.colab import userdata

# Fetch API key from Colab secrets
client = OpenAI(api_key="<custom-open-api-key>")

def get_embedding(text, model="text-embedding-ada-002"):
   text = text.replace("\n", " ")
   return client.embeddings.create(input = [text],
model=model).data[0].embedding

ada_embeddings_llm = []
for i in range(len(llm)):
    #print(i)
    ada_embeddings_llm.append(get_embedding(llm.loc[i, "code"], model='text-embedding-ada-002'))
llm["ada_embedding"] = ada_embeddings_llm

ada_embeddings_human = []
for i in range(len(human)):
    #print(i)
    ada_embeddings_human.append(get_embedding(human.loc[i, "code"], model='text-embedding-ada-002'))
human["ada_embedding"] = ada_embeddings_human
human

### Detailed Analysis of Results

The table below shows the performance metrics (F1 Score, Accuracy, Precision, and Recall) for each classifier (Random Forest, XGBoost, and SVM) using both Ada embeddings and TF-IDF features, evaluated on different subsets of the test data (Human-written code, LLM-written code broken down by model, and different submission statuses). The Naive Bayes-like method is also included for comparison.

Let's visualize the performance metrics for each model and feature combination across the different test sets. This will help us see how well each approach performs on distinguishing between human and LLM-generated code under various conditions.

training_llm = llm[["problem_id","submission_id", "status_in_folder","LLM", "code", "ada_embedding","label",]]
training_human = human[["problem_id","submission_id", "status_in_folder","LLM", "code", "ada_embedding","label",]]

all_data = pd.concat([training_llm, training_human])
all_data

import re
def stylometric_python_code(python_code):
    lines = python_code.split("\n")

    number_of_code_lines = 0
    number_of_comments = 0
    number_of_functions = 0
    number_of_blank_lines = 0

    in_multiline_comment = False

    for line in lines:
        number_of_code_lines += 1
        stripped = line.rstrip()  # Preserve blank lines properly

        # Blank line detection
        if stripped == "":
            number_of_blank_lines += 1
            continue

        # Multi-line comment (docstring) detection
        if in_multiline_comment:
            number_of_comments += 1
            if '"""' in stripped or "'''" in stripped:
                in_multiline_comment = False
            continue

        # Single-line comment detection
        if stripped.startswith("#"):
            number_of_comments += 1
            continue

        # Multi-line comment start (detect triple quotes)
        if stripped.startswith(('"""', "'''")):
            in_multiline_comment = True
            number_of_comments += 1  # Count the whole docstring block as a comment
            if stripped.endswith(('"""', "'''")):
                in_multiline_comment = False
            continue

        # Detect inline comments correctly (avoid '#' inside strings)
        in_string = False
        comment_index = None
        for i, char in enumerate(stripped):
            if char in "\"'" and (i == 0 or stripped[i - 1] != "\\"):  # Detect string start (ignoring escaped quotes)
                in_string = not in_string  # Toggle in_string mode

            if char == "#" and not in_string:
                comment_index = i
                break  # Found valid comment outside a string

        if comment_index is not None:
            number_of_comments += 1
            stripped = stripped[:comment_index].rstrip()  # Remove comment part for further checks

    pattern = r"^\s*def\s+([\w]+)\s*\("

    matches = re.findall(pattern, python_code, re.MULTILINE)
    number_of_functions = len(matches)

    return {
        "number_of_code_lines": number_of_code_lines,
        "number_of_comments": number_of_comments,
        "number_of_functions": number_of_functions,
        "number_of_blank_lines": number_of_blank_lines
    }

def run_code(detect_data):
    """
    Applies the 'stylometric_python_code' function to each code sample in the DataFrame.
    Calculates and adds new stylometric features as columns.
    """

    # Apply the function directly to the 'code' column, returns dictionaries
    results = detect_data['code'].apply(stylometric_python_code)

    # Extract each metric into new columns using list comprehension and lambda
    detect_data['lines'] = results.apply(lambda x: x['number_of_code_lines'])
    detect_data['code_lines'] = results.apply(lambda x: x['number_of_code_lines']-x["number_of_blank_lines"])
    detect_data['comments'] = results.apply(lambda x: x['number_of_comments'])
    detect_data['functions'] = results.apply(lambda x: x['number_of_functions'])
    detect_data['blank_lines'] = results.apply(lambda x: x['number_of_blank_lines'])


    return detect_data

new_all = run_code(all_data)
new_all

new_all.to_csv("all_data_with_ada_embeddings_will_be_splitted_into_train_test_set.csv")

new_all

human_written = new_all[new_all["LLM"] == "Human"]
llm_written = new_all[new_all["LLM"] != "Human"]

print(human_written["code_lines"].mean())
print(human_written["blank_lines"].mean())
print(human_written["comments"].mean())
print(human_written["functions"].mean())

print(llm_written[llm_written["LLM"]=="LLAMA"]["code_lines"].mean())
print(llm_written[llm_written["LLM"]=="LLAMA"]["blank_lines"].mean())
print(llm_written[llm_written["LLM"]=="LLAMA"]["comments"].mean())
print(llm_written[llm_written["LLM"]=="LLAMA"]["functions"].mean())

print(llm_written[llm_written["LLM"]=="GEMINI"]["code_lines"].mean())
print(llm_written[llm_written["LLM"]=="GEMINI"]["blank_lines"].mean())
print(llm_written[llm_written["LLM"]=="GEMINI"]["comments"].mean())
print(llm_written[llm_written["LLM"]=="GEMINI"]["functions"].mean())

print(llm_written[llm_written["LLM"]=="CODESTRAL"]["code_lines"].mean())
print(llm_written[llm_written["LLM"]=="CODESTRAL"]["blank_lines"].mean())
print(llm_written[llm_written["LLM"]=="CODESTRAL"]["comments"].mean())
print(llm_written[llm_written["LLM"]=="CODESTRAL"]["functions"].mean())

data = pd.read_csv("all_data_with_ada_embeddings_will_be_splitted_into_train_test_set.csv")
data

import ast
data["ada_embedding_vector"] = data.ada_embedding.apply(lambda x: ast.literal_eval(x))
data.head()

from sklearn.model_selection import train_test_split
"""
    Train-test Split
"""
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)
print(train_data.shape)
print(test_data.shape)

test_data.groupby("LLM").count()

train_data.groupby("LLM").count()

import matplotlib.pyplot as plt
import numpy as np
tst_dt_grpd = test_data.groupby(["LLM","status_in_folder"]).count()
tst_dt_grpd = tst_dt_grpd.reset_index()[["LLM","status_in_folder","code"]]

# Create figure and axis
plt.figure(figsize=(15,7))

# Get LLM distribution data
llm_data = tst_dt_grpd.groupby('LLM')['code'].sum()

# Create pie chart
plt.pie(llm_data, labels=llm_data.index, autopct='%1.1f%%')
plt.title('Distribution of LLMs in TEST SET')

# Get status distribution by LLM
status_by_llm = tst_dt_grpd.groupby(['LLM', 'status_in_folder'])['code'].sum()

# Calculate angles for text placement
angles = {}
start_angle = 0
for llm, count in llm_data.items():
    # Calculate middle angle for this LLM segment
    angle = start_angle + (count/llm_data.sum() * 360)/2
    angles[llm] = angle
    start_angle += (count/llm_data.sum() * 360)

# Add text annotations with arrows
for llm in llm_data.index:
    angle = angles[llm]
    rad = angle * (np.pi/180)  # Convert to radians

    # Calculate text position
    x = 1.5 * np.cos(rad)
    y = 1.5 * np.sin(rad)

    # Create status breakdown text
    status_text = f"\n{llm} breakdown:\n"
    for status in status_by_llm[llm].index:
        pct = (status_by_llm[llm][status] / llm_data[llm]) * 100
        status_text += f"{status}: {pct:.1f}%\n"

    # Add arrow and text
    plt.annotate(status_text,
                xy=(0.8*np.cos(rad), 0.8*np.sin(rad)),
                xytext=(x, y),
                arrowprops=dict(arrowstyle="->"))

plt.axis('equal')

import matplotlib.pyplot as plt
import numpy as np
trn_dt_grpd = train_data.groupby(["LLM","status_in_folder"]).count()
trn_dt_grpd = trn_dt_grpd.reset_index()[["LLM","status_in_folder","code"]]

# Create figure and axis
plt.figure(figsize=(15,7))

# Get LLM distribution data
llm_data = trn_dt_grpd.groupby('LLM')['code'].sum()

# Create pie chart
plt.pie(llm_data, labels=llm_data.index, autopct='%1.1f%%')
plt.title('Distribution of LLMs in TRAIN SET')

# Get status distribution by LLM
status_by_llm = trn_dt_grpd.groupby(['LLM', 'status_in_folder'])['code'].sum()

# Calculate angles for text placement
angles = {}
start_angle = 0
for llm, count in llm_data.items():
    # Calculate middle angle for this LLM segment
    angle = start_angle + (count/llm_data.sum() * 360)/2
    angles[llm] = angle
    start_angle += (count/llm_data.sum() * 360)

# Add text annotations with arrows
for llm in llm_data.index:
    angle = angles[llm]
    rad = angle * (np.pi/180)  # Convert to radians

    # Calculate text position
    x = 1.5 * np.cos(rad)
    y = 1.5 * np.sin(rad)

    # Create status breakdown text
    status_text = f"\n{llm} breakdown:\n"
    for status in status_by_llm[llm].index:
        pct = (status_by_llm[llm][status] / llm_data[llm]) * 100
        status_text += f"{status}: {pct:.1f}%\n"

    # Add arrow and text
    plt.annotate(status_text,
                xy=(0.8*np.cos(rad), 0.8*np.sin(rad)),
                xytext=(x, y),
                arrowprops=dict(arrowstyle="->"))

plt.axis('equal')

from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score
def print_metric_scores(y_test, y_pred):
    """
        Calculates and prints F1-score, accuracy, precision, and recall for classification results.
    """
    f1 = "{:.4f}".format(f1_score(y_test, y_pred))
    acc = "{:.4f}".format(accuracy_score(y_test, y_pred))
    pre = "{:.4f}".format(precision_score(y_test, y_pred))
    rec = "{:.4f}".format(recall_score(y_test, y_pred))
    print("f1:",f1, "   acc:", acc, "   pre:", pre, "   rec:", rec)

from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer
import xgboost as xgb
import numpy as np
from sklearn.ensemble import RandomForestClassifier

# Initialize TF-IDF vectorizer to convert text to feature vectors (top 5000 terms)
tfidf = TfidfVectorizer(max_features=5000)

def train_xgb(xtrain, ytrain, xtest, ytest):
    """
    Trains an XGBoost classifier and prints evaluation metrics.
    """
    xgb_clf = xgb.XGBClassifier(
        objective='binary:logistic',
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        random_state=42
    )
    xgb_clf.fit(xtrain, ytrain)
    y_pred = xgb_clf.predict(xtest)
    print_metric_scores(ytest, y_pred)
    return xgb_clf

def train_rf(X_train, X_test, y_train, y_test):
    """
    Trains a Random Forest classifier and prints evaluation metrics.
    """
    rf_classifier = RandomForestClassifier()
    rf_classifier.fit(X_train, y_train)
    y_pred = rf_classifier.predict(X_test)
    print_metric_scores(y_test, y_pred)
    return rf_classifier

def train_svm(X_train, X_test, y_train, y_test):
    """
    Trains an SVM classifier and prints evaluation metrics.
    """
    svm_classifier = SVC()
    svm_classifier.fit(X_train, y_train)
    y_pred = svm_classifier.predict(X_test)
    print_metric_scores(y_test, y_pred)
    return svm_classifier

# Prepare Ada embedding vectors as input features
X_train_ada = np.stack(train_data["ada_embedding_vector"].values)
X_test_ada = np.stack(test_data["ada_embedding_vector"].values)

# Prepare TF-IDF features from code texts
X_train_tf_idf = tfidf.fit_transform(train_data['code'])
X_test_tf_idf = tfidf.transform(test_data['code'])

# Extract labels for training and testing
y_train = train_data['label']
y_test = test_data['label']

print("XGB")
print("\t TF-IDF")
xgb_classifier_trained_on_tf_idf = train_xgb(X_train_tf_idf, y_train, X_test_tf_idf, y_test)
print("\t ADA")
xgb_classifier_trained_on_ada_embeddings = train_xgb(X_train_ada, y_train, X_test_ada, y_test)
print("-------------------------------------------------------------")
print("RF")
print("\t TF-IDF")
rf_classifier_trained_on_tf_idf = train_rf(X_train_tf_idf, X_test_tf_idf, y_train, y_test)
print("\t ADA")
rf_classifier_trained_on_ada_embeddings = train_rf(X_train_ada, X_test_ada, y_train, y_test)
print("-------------------------------------------------------------")
print("SVM")
print("\t TF-IDF")
svm_classifier_trained_on_tf_idf = train_svm(X_train_tf_idf, X_test_tf_idf, y_train, y_test)
print("\t ADA")
svm_classifier_trained_on_ada_embeddings = train_svm(X_train_ada, X_test_ada, y_train, y_test)

# --- Split test data based on code author (LLM or Human) ---
human_test = test_data[test_data["LLM"] == "Human"]
llama_test = test_data[test_data["LLM"] == "LLAMA"]
gemini_test = test_data[test_data["LLM"] == "GEMINI"]
codestral_test = test_data[test_data["LLM"] == "CODESTRAL"]

# --- Split test data based on submission status ---
test_accepted_and_generated = test_data[test_data["status_in_folder"].isin(["Accepted", "Generate"])]
test_wrong = test_data[test_data["status_in_folder"] == "Wrong"]
test_runtime = test_data[test_data["status_in_folder"] == "Runtime"]

# --- Create mixed subsets to analyze specific scenarios ---
# Wrong codes by LLMs + Accepted codes by Humans
test_wrong_llm_and_accepted_human = test_data[
    ((test_data["status_in_folder"] == "Wrong") & (test_data["LLM"] != "Human")) |
    ((test_data["status_in_folder"] == "Accepted") & (test_data["LLM"] == "Human"))
]

# Runtime errors by LLMs + Accepted codes by Humans
test_runtime_llm_and_accepted_human = test_data[
    ((test_data["status_in_folder"] == "Runtime") & (test_data["LLM"] != "Human")) |
    ((test_data["status_in_folder"] == "Accepted") & (test_data["LLM"] == "Human"))
]

# --- Grouping datasets for evaluation ---
# Grouped by author
test_data_author_tuple = [
    ("Human", human_test),
    ("LLAMA", llama_test),
    ("Gemini", gemini_test),
    ("Codestral", codestral_test)
]

# Grouped by code status
test_data_status_tuple = [
    ("Accepted+Generated", test_accepted_and_generated),
    ("Wrong", test_wrong),
    ("Runtime", test_runtime)
]


# Full test set
test_all = [("All Test Set", test_data)]

# Combined all splits for evaluation loops
test_data_tuple_all = test_data_author_tuple + test_data_status_tuple + test_all

def get_results_of_classifier_feature_combination(classifier_name_in_str_format, classifier, feature_extractor, test_data_tuple):
    """
    Evaluates a given classifier with a specified feature set across multiple test subsets.

    Parameters:
    - classifier_name_in_str_format (str): Classifier name for result labeling (e.g., 'RF', 'XGB', 'SVM').
    - classifier: Pre-trained classifier object.
    - feature_extractor (str): Feature type ('ada' or 'tfidf').
    - test_data_tuple (list of tuples): List containing test set name and corresponding DataFrame.

    Returns:
    - pd.DataFrame: Evaluation results with F1, Accuracy, Precision, Recall per test subset.
    """
    results = []
    for test_set, test_data_subset in test_data_tuple:

        feature_extractor_name = ""

        # Select features and make predictions based on extractor type
        if feature_extractor == "ada":
            y_pred = classifier.predict(np.stack(test_data_subset["ada_embedding_vector"].values))
            feature_extractor_name = "Ada Embeddings"
        elif feature_extractor == "tfidf":
            y_pred = classifier.predict(tfidf.transform(test_data_subset['code']))
            feature_extractor_name = "TF-IDF Vector"

        # Calculate evaluation metrics
        f1 = f1_score(test_data_subset['label'], y_pred)
        accuracy = accuracy_score(test_data_subset['label'], y_pred)
        precision = precision_score(test_data_subset['label'], y_pred)
        recall = recall_score(test_data_subset['label'], y_pred)

        # Append results for this test set
        results.append({
            'Model': classifier_name_in_str_format,
            'Features': feature_extractor_name,
            'Test Set': test_set,
            'F1 Score': f1,
            'Accuracy': accuracy,
            'Precision': precision,
            'Recall': recall
        })

    return pd.DataFrame(results)

# --- Run evaluation for all classifiers and feature combinations ---
results_of_rf_ada = get_results_of_classifier_feature_combination("RF", rf_classifier_trained_on_ada_embeddings, "ada", test_data_tuple_all)
results_of_rf_tfidf = get_results_of_classifier_feature_combination("RF", rf_classifier_trained_on_tf_idf, "tfidf", test_data_tuple_all)

results_of_xgb_tfidf = get_results_of_classifier_feature_combination("XGB", xgb_classifier_trained_on_tf_idf, "tfidf", test_data_tuple_all)
results_of_xgb_ada = get_results_of_classifier_feature_combination("XGB", xgb_classifier_trained_on_ada_embeddings, "ada", test_data_tuple_all)

results_of_svm_tfidf = get_results_of_classifier_feature_combination("SVM", svm_classifier_trained_on_tf_idf, "tfidf", test_data_tuple_all)
results_of_svm_ada = get_results_of_classifier_feature_combination("SVM", svm_classifier_trained_on_ada_embeddings, "ada", test_data_tuple_all)

# Combine all results into a single DataFrame
all_results = pd.concat([
    results_of_rf_ada, results_of_rf_tfidf,
    results_of_xgb_tfidf, results_of_xgb_ada,
    results_of_svm_tfidf, results_of_svm_ada
])

all_results.reset_index(drop=True)

import tiktoken
import numpy as np
from collections import Counter

### Obtained from: https://github.com/MarcOedingen/ChatGPT-Code-Detection/blob/main/Bayes_Classifier/bayes_class.py
def classify_tokenized_code(
    tokenized_code, tokens_above_tau, prob_tk_given_H, prob_tk_given_G, P_H=0.5, P_G=0.5
):
    filtered_tokens = [token for token in tokenized_code if token in tokens_above_tau]

    log_prob_sum_H = np.sum(
        [np.log(prob_tk_given_H[token]) for token in filtered_tokens]
    )
    log_prob_sum_G = np.sum(
        [np.log(prob_tk_given_G[token]) for token in filtered_tokens]
    )

    log_prob_H_numerator = np.log(P_H) + log_prob_sum_H
    log_prob_G_numerator = np.log(P_G) + log_prob_sum_G

    log_denominator = np.logaddexp(log_prob_H_numerator, log_prob_G_numerator)

    log_prob_H = log_prob_H_numerator - log_denominator
    log_prob_G = log_prob_G_numerator - log_denominator

    return 0 if log_prob_H > log_prob_G else 1


def run_on_problems(train, test):
    X_train = train[["code","label"]]
    X_test = test[["code","label"]]

    y_test = test["label"]

    tokenizer = tiktoken.get_encoding("cl100k_base")
    tau = (len(X_train)+len(X_test)) / int(1e3)

    human_train = X_train[X_train["label"] == 0]["code"]
    llm_train = X_train[X_train["label"] == 1]["code"]

    tokenized_test = X_test["code"].apply(lambda x: tokenizer.encode(x))
    tokenized_human_train = human_train.apply(lambda x: tokenizer.encode(x))
    tokenized_llm_train = llm_train.apply(lambda x: tokenizer.encode(x))

    flat_human_tokens = [
        token for sublist in tokenized_human_train for token in sublist
    ]
    flat_llm_tokens = [token for sublist in tokenized_llm_train for token in sublist]

    human_counter = Counter(flat_human_tokens)
    llm_counter = Counter(flat_llm_tokens)

    human_counter_tau = {k: v for k, v in human_counter.items() if v > tau}
    llm_counter_tau = {k: v for k, v in llm_counter.items() if v > tau}

    tokens_above_tau = set(human_counter_tau.keys()).intersection(
        set(llm_counter_tau.keys())
    )
    total_human_tokens = sum(human_counter_tau.values())
    total_llm_tokens = sum(llm_counter_tau.values())
    prob_tk_given_H = {
        token: human_counter_tau[token] / total_human_tokens
        for token in tokens_above_tau
    }
    prob_tk_given_G = {
        token: llm_counter_tau[token] / total_llm_tokens for token in tokens_above_tau
    }

    predictions = tokenized_test.apply(
        lambda x: classify_tokenized_code(
            x, tokens_above_tau, prob_tk_given_H, prob_tk_given_G
        )
    )

    y_pred = predictions
    #y_prob = predictions
    #print_metric_scores(y_test, y_pred)
    return y_pred

# Evaluate Naive Bayes-like method (run_on_problems) on all predefined test splits
results_bayes = []

for test_name, dt in test_data_tuple_all:
    #print(test_name)
    y_pred = run_on_problems(train_data, dt)  # Custom prediction function on raw code

    # Append results as dict per test set
    results_bayes.append({
        'Model': "BAYES",
        'Features': "Code itself",
        'Test Set': test_name,
        'F1 Score': f1_score(dt["label"], y_pred),
        'Accuracy': accuracy_score(dt["label"], y_pred),
        'Precision': precision_score(dt["label"], y_pred),
        'Recall': recall_score(dt["label"], y_pred)
    })

# Convert to DataFrame
res_bayes = pd.DataFrame(results_bayes)
res_bayes

all_results_with_bayes = pd.concat([all_results, res_bayes]).reset_index(drop=True)
all_results_with_bayes

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# y_test = your true labels
# y_pred = predictions from your trained model
cm = confusion_matrix(y_test, y_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap='Blues')   # you can change color map if you like
plt.title("Confusion Matrix â€“ XGBoost Classifier")
plt.show()

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import joblib
import matplotlib.pyplot as plt
import io

# Path to your combined malicious + benign dataset
malicious_dataset_path = "combined_structured_malicious_benign.xlsx"

# Column containing the code text
CODE_COL = "setup_content"

# ----------------------------
# Load and preprocess data
# ----------------------------
df_mal = pd.read_excel(malicious_dataset_path)
df_mal = df_mal.dropna(subset=[CODE_COL]).reset_index(drop=True)

X_texts = df_mal[CODE_COL].astype(str)
y_labels = df_mal["label"].astype(int)  # 1 = malicious, 0 = benign

print(f"Loaded dataset with {len(df_mal)} samples")
print(df_mal["label"].value_counts())

# ----------------------------
# TF-IDF Feature Extraction
# ----------------------------
tfidf_malicious = TfidfVectorizer(
    max_features=5000,
    token_pattern=r"\b\w+\b",
    lowercase=True,
    ngram_range=(1, 2)
)
X = tfidf_malicious.fit_transform(X_texts)

# ----------------------------
# Train/Test Split
# ----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y_labels, stratify=y_labels, test_size=0.2, random_state=42
)

# ----------------------------
# Model Training
# ----------------------------
malicious_detector = RandomForestClassifier(
    n_estimators=300,
    n_jobs=-1,
    random_state=42
)
malicious_detector.fit(X_train, y_train)


# ----------------------------
# Evaluation
# ----------------------------
y_pred = malicious_detector.predict(X_test)

print("\n=== Malicious Detector Evaluation ===")

# Capture the classification report output
report_str = classification_report(y_test, y_pred, digits=3, output_dict=True)
report_df = pd.DataFrame(report_str).transpose()
display(report_df)

cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap='Blues')
plt.title("Malicious Code Classifier - Confusion Matrix")
plt.show()

# ----------------------------
# Save Model & Vectorizer
# ----------------------------
joblib.dump(malicious_detector, "malicious_detector.pkl")
joblib.dump(tfidf_malicious, "tfidf_malicious.pkl")

print("\n Models saved successfully:")
print(" - malicious_detector.pkl")
print(" - tfidf_malicious.pkl")

# ====================================================
# TWO-STAGE PIPELINE with Human-Readable Confidence
# ====================================================

import numpy as np


def interpret_confidence(value):
    """Convert numeric confidence (0â€“1) to readable label."""
    if value is None:
        return "N/A"
    if value < 0.5:
        return f"{value:.3f} â†’ Uncertain"
    elif value < 0.7:
        return f"{value:.3f} â†’ Moderate confidence"
    elif value < 0.86:
        return f"{value:.3f} â†’ High confidence"
    else:
        return f"{value:.3f} â†’ Very high confidence"


def two_stage_predict(code_text, author_model, malicious_vectorizer, author_vectorizer):
    """
    Two-stage pipeline:
    Stage 1 â†’ Malicious vs Benign classification
    Stage 2 â†’ Authorship identification (only if malicious)
    """

    # Map numeric author labels to text
    author_label_map = {
        0: "Human",
        1: "AI-generated",
        "0": "Human",
        "1": "AI-generated"
    }

    result = {
        "malicious": None,
        "malicious_confidence": None,
        "author": None,
        "author_confidence": None
    }

    # ---------- Stage 1: Malicious Detection ----------
    X_m = malicious_vectorizer.transform([code_text])
    mal_pred = malicious_detector.predict(X_m)[0]
    mal_conf = None
    if hasattr(malicious_detector, "predict_proba"):
        mal_conf = float(np.max(malicious_detector.predict_proba(X_m)))

    result["malicious"] = bool(mal_pred)
    result["malicious_confidence"] = mal_conf

    # ---------- Stage 2: Authorship Identification ----------
    if mal_pred == 1:
        X_a = author_vectorizer.transform([code_text])
        author_pred = author_model.predict(X_a)[0]
        author_conf = None
        if hasattr(author_model, "predict_proba"):
            author_conf = float(np.max(author_model.predict_proba(X_a)))

        author_name = author_label_map.get(author_pred, str(author_pred))
        result["author"] = author_name
        result["author_confidence"] = author_conf

    # ---------- Result Print ----------
    print("\n=== Classification Result ===")
    print(f"Malicious: {result['malicious']}  (Confidence: {interpret_confidence(result['malicious_confidence'])})")

    if result["malicious"]:
        print(f"â†’ Author: {result['author']}  (Confidence: {interpret_confidence(result['author_confidence'])})")
    else:
        print("â†’ Code classified as benign.")
    print("================================\n")

    return result

# ==============================================
# ðŸ§‘â€ðŸ’» INTERACTIVE CODE ANALYZER (Colab Widget)
# ==============================================

import ipywidgets as widgets
from IPython.display import display, clear_output

# Create a large text area for code input
code_input = widgets.Textarea(
    value='',
    placeholder='Paste or type your code here...',
    description='Code:',
    layout=widgets.Layout(width='100%', height='200px')
)

# Create a button to trigger analysis
analyze_button = widgets.Button(
    description='Analyze Code',
    button_style='success',
    tooltip='Click to analyze the entered code'
)

# Output area
output_area = widgets.Output()

# Define the button click behavior
def on_analyze_click(b):
    with output_area:
        clear_output()
        code_text = code_input.value.strip()
        if not code_text:
            print("Please enter some code to analyze.")
            return

        result = two_stage_predict(code_text, rf_classifier_trained_on_tf_idf, tfidf_malicious, tfidf)


# Link button to function
analyze_button.on_click(on_analyze_click)

# Display UI elements
display(code_input, analyze_button, output_area)
